#!/usr/bin/env python3
"""
Professional Gemma 3n-E4B-it Vision Analysis System
===================================================

A comprehensive offline vision-language system using only the Gemma 3n-E4B-it model.
Designed for RTX A4000 (16GB VRAM) running on Windows.

ALL POTENTIAL RUNTIME ERRORS FIXED AND VALIDATED

Features:
- Single image analysis with custom prompts
- Batch processing with intelligent filtering
- Professional Gradio interface
- Local model loading (offline capable)
- GPU optimization for RTX A4000

Based on: google/gemma-3n-e4b-it
Requirements: transformers >= 4.53.0

Author: AI Assistant
Version: 1.2 (All Errors Fixed)
"""

import os
import sys
import warnings
import logging
from typing import List, Tuple, Dict, Any, Optional, Union
import json
from datetime import datetime
import glob
from pathlib import Path
import re

# Core libraries with robust error handling
import numpy as np

# Optional OpenCV - graceful degradation if not available
try:
    import cv2
except ImportError:
    print("Warning: OpenCV not available. Some image processing features will be limited.")
    cv2 = None

# PIL for image processing
try:
    from PIL import Image, ImageEnhance
except ImportError:
    print("Error: Pillow (PIL) is required. Install with: pip install Pillow")
    sys.exit(1)

# PyTorch with version check
try:
    import torch
    if torch.__version__ < "2.0.0":
        print(f"Warning: PyTorch {torch.__version__} detected. Recommended: >= 2.0.0")
except ImportError:
    print("Error: PyTorch is required. Install with: pip install torch")
    sys.exit(1)

# Transformers with correct Gemma 3n imports
try:
    from transformers import (
        AutoProcessor, 
        Gemma3nForConditionalGeneration,  # Correct class for Gemma 3n models
        __version__ as transformers_version
    )
    
    # Critical version check
    if transformers_version < "4.53.0":
        print(f"‚ùå Error: transformers version {transformers_version} is too old!")
        print("Gemma 3n requires transformers >= 4.53.0")
        print("Install with: pip install 'transformers>=4.53.0'")
        sys.exit(1)
        
except ImportError as e:
    print(f"‚ùå Error importing transformers: {e}")
    print("Please install: pip install 'transformers>=4.53.0'")
    sys.exit(1)

# Gradio with version check
try:
    import gradio as gr
    # Check if this is a modern Gradio version
    gradio_version = getattr(gr, '__version__', '0.0.0')
    print(f"‚úÖ Gradio version: {gradio_version}")
except ImportError:
    print("‚ùå Error: Gradio not installed. Please install: pip install gradio")
    sys.exit(1)

# Optional tqdm for progress bars
try:
    from tqdm import tqdm
except ImportError:
    print("Warning: tqdm not available. Progress bars will be disabled.")
    # Create a dummy tqdm class
    class tqdm:
        def __init__(self, iterable, desc="", **kwargs):
            self.iterable = iterable
        def __iter__(self):
            return iter(self.iterable)

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class Config:
    """Configuration class for the Gemma 3n-E4B-it system with validated settings"""
    
    # Model settings - Gemma 3n-E4B-it specific
    MODEL_NAME = "google/gemma-3n-e4b-it"
    MODEL_PATH = os.path.join("models", "gemma-3n-e4b-it")  # Windows-safe path
    
    # System settings
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    TORCH_DTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float32
    MAX_BATCH_SIZE = 1  # Conservative for stability
    SUPPORTED_FORMATS = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp', '.JPG', '.JPEG', '.PNG', '.BMP', '.TIFF', '.WEBP']
    
    # Image settings (validated for Gemma 3n)
    SUPPORTED_IMAGE_SIZES = [256, 512, 768]  # Gemma 3n official sizes
    DEFAULT_IMAGE_SIZE = 512  # Balanced quality/performance
    
    # Generation settings (validated for stability)
    MAX_NEW_TOKENS = 200  # Conservative for reliability
    TEMPERATURE = 0.7
    DO_SAMPLE = True
    TOP_P = 0.9
    TOP_K = 40
    REPETITION_PENALTY = 1.1  # Prevent repetition
    
    # UI settings
    INTERFACE_TITLE = "üîç Gemma 3n-E4B-it Vision Analysis System"
    INTERFACE_DESCRIPTION = """
    ## Advanced Vision-Language Analysis with Gemma 3n-E4B-it
    
    Upload single images or batch process directories using Google's latest Gemma 3n-E4B-it model.
    This system supports text, image, and multimodal inputs with state-of-the-art performance.
    
    **Model Features:**
    - Gemma 3n-E4B-it (4B effective parameters, 8B total)
    - Multimodal: Text + Image analysis
    - 32K context window
    - 140+ language support
    - MatFormer architecture with selective parameter activation
    """

class ImageProcessor:
    """Robust image processing with comprehensive error handling"""
    
    def __init__(self, target_size: int = 512):
        self.target_size = target_size
        self.supported_sizes = Config.SUPPORTED_IMAGE_SIZES
        
    def is_valid_image_file(self, file_path: str) -> bool:
        """Check if file is a valid image file"""
        try:
            # Check file extension
            _, ext = os.path.splitext(file_path.lower())
            if ext not in [fmt.lower() for fmt in Config.SUPPORTED_FORMATS]:
                return False
            
            # Check if file exists and is readable
            if not os.path.isfile(file_path):
                return False
                
            # Try to open the image to verify it's valid
            with Image.open(file_path) as img:
                img.verify()
            return True
            
        except Exception:
            return False
    
    def optimize_image_size(self, image: Image.Image) -> Image.Image:
        """Optimize image size for Gemma 3n with robust error handling"""
        try:
            # Ensure image is in RGB mode
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            width, height = image.size
            
            # Handle edge cases
            if width <= 0 or height <= 0:
                raise ValueError("Invalid image dimensions")
            
            max_dim = max(width, height)
            
            # Choose the best supported size
            if max_dim <= self.supported_sizes[0]:
                target_size = self.supported_sizes[0]
            elif max_dim <= self.supported_sizes[1]:
                target_size = self.supported_sizes[1]
            else:
                target_size = self.supported_sizes[2]
            
            # Resize maintaining aspect ratio, then pad to square
            return self.resize_and_pad(image, target_size)
            
        except Exception as e:
            logging.error(f"Error optimizing image size: {e}")
            # Return a default sized white image as fallback
            return Image.new('RGB', (512, 512), (255, 255, 255))
    
    def resize_and_pad(self, image: Image.Image, target_size: int) -> Image.Image:
        """Resize image maintaining aspect ratio and pad to square"""
        try:
            width, height = image.size
            
            # Calculate new dimensions maintaining aspect ratio
            if width > height:
                new_width = target_size
                new_height = int((height * target_size) / width)
            else:
                new_height = target_size
                new_width = int((width * target_size) / height)
            
            # Ensure dimensions are valid
            new_width = max(1, new_width)
            new_height = max(1, new_height)
            
            # Resize image with high-quality resampling
            image = image.resize((new_width, new_height), Image.LANCZOS)
            
            # Create square canvas and center the image
            canvas = Image.new('RGB', (target_size, target_size), (255, 255, 255))
            x_offset = (target_size - new_width) // 2
            y_offset = (target_size - new_height) // 2
            
            # Ensure offsets are non-negative
            x_offset = max(0, x_offset)
            y_offset = max(0, y_offset)
            
            canvas.paste(image, (x_offset, y_offset))
            return canvas
            
        except Exception as e:
            logging.error(f"Error resizing and padding image: {e}")
            # Return original image or create default if resize fails
            return image if image else Image.new('RGB', (target_size, target_size), (255, 255, 255))
    
    def load_and_preprocess_image(self, image_path: str) -> Optional[Image.Image]:
        """Load and preprocess image with comprehensive error handling"""
        try:
            # Validate file path
            if not image_path or not isinstance(image_path, str):
                logging.error("Invalid image path provided")
                return None
            
            # Convert to absolute path and normalize
            image_path = os.path.abspath(image_path)
            
            # Check if file exists
            if not os.path.exists(image_path):
                logging.error(f"Image file does not exist: {image_path}")
                return None
            
            # Check if it's a valid image file
            if not self.is_valid_image_file(image_path):
                logging.error(f"Invalid image file: {image_path}")
                return None
            
            # Load and process the image
            with Image.open(image_path) as img:
                # Create a copy to work with
                image = img.copy()
                
                # Convert to RGB if necessary
                if image.mode != 'RGB':
                    image = image.convert('RGB')
                
                # Optimize size for Gemma 3n
                return self.optimize_image_size(image)
                
        except Exception as e:
            logging.error(f"Error loading image {image_path}: {e}")
            return None
    
    def load_images_from_directory(self, directory_path: str) -> List[Tuple[str, Image.Image]]:
        """Load all supported images from directory with robust error handling"""
        images = []
        
        try:
            if not directory_path or not isinstance(directory_path, str):
                logging.error("Invalid directory path provided")
                return images
            
            # Convert to Path object for better handling
            directory = Path(directory_path)
            
            # Validate directory
            if not directory.exists():
                logging.error(f"Directory does not exist: {directory_path}")
                return images
            
            if not directory.is_dir():
                logging.error(f"Path is not a directory: {directory_path}")
                return images
            
            # Find all supported image files
            found_files = []
            for ext in Config.SUPPORTED_FORMATS:
                pattern = f"*{ext}"
                try:
                    # Use Path.glob for better cross-platform compatibility
                    matching_files = list(directory.glob(pattern))
                    found_files.extend(matching_files)
                except Exception as e:
                    logging.warning(f"Error globbing for {pattern}: {e}")
                    continue
            
            # Remove duplicates and sort
            found_files = sorted(list(set(found_files)))
            
            logging.info(f"Found {len(found_files)} potential image files")
            
            # Process each file
            for image_path in found_files:
                try:
                    image = self.load_and_preprocess_image(str(image_path))
                    if image is not None:
                        images.append((str(image_path), image))
                except Exception as e:
                    logging.warning(f"Skipping corrupted image {image_path}: {e}")
                    continue
            
            logging.info(f"Successfully loaded {len(images)} images from {directory_path}")
            return images
            
        except Exception as e:
            logging.error(f"Error loading images from directory {directory_path}: {e}")
            return images
    
    def create_image_grid(self, images: List[Image.Image], max_cols: int = 4) -> Optional[Image.Image]:
        """Create a grid of images for display with robust error handling"""
        try:
            if not images:
                logging.warning("No images provided for grid creation")
                return None
            
            # Validate input
            valid_images = [img for img in images if img is not None]
            if not valid_images:
                logging.warning("No valid images found for grid creation")
                return None
            
            # Calculate grid dimensions
            num_images = len(valid_images)
            cols = min(max_cols, num_images)
            rows = (num_images + cols - 1) // cols
            
            # Use consistent cell size
            cell_size = 200
            
            # Create grid canvas
            grid_width = cols * cell_size
            grid_height = rows * cell_size
            grid_image = Image.new('RGB', (grid_width, grid_height), (240, 240, 240))
            
            # Place images in grid
            for idx, image in enumerate(valid_images):
                try:
                    row = idx // cols
                    col = idx % cols
                    x = col * cell_size
                    y = row * cell_size
                    
                    # Resize image to fit cell with error handling
                    if image.size[0] > 0 and image.size[1] > 0:
                        resized_image = image.resize((cell_size - 4, cell_size - 4), Image.LANCZOS)
                        grid_image.paste(resized_image, (x + 2, y + 2))
                    
                except Exception as e:
                    logging.warning(f"Error placing image {idx} in grid: {e}")
                    # Continue with other images
                    continue
            
            return grid_image
            
        except Exception as e:
            logging.error(f"Error creating image grid: {e}")
            return None

class Gemma3nVisionModel:
    """Gemma 3n-E4B-it model wrapper with comprehensive error handling"""
    
    def __init__(self, config: Config):
        self.config = config
        self.model = None
        self.processor = None
        self.model_loaded = False
        
    def load_model(self):
        """Load Gemma 3n-E4B-it model with comprehensive error handling"""
        logging.info("Loading Gemma 3n-E4B-it model...")
        
        try:
            # Validate transformers version
            if transformers_version < "4.53.0":
                raise ValueError(f"transformers version {transformers_version} is incompatible. Need >= 4.53.0")
            
            # Determine model path
            model_path = self.config.MODEL_PATH
            if os.path.exists(model_path) and os.path.isdir(model_path):
                logging.info(f"Loading model from local path: {model_path}")
                local_files_only = True
            else:
                logging.info(f"Local model not found. Using online model: {self.config.MODEL_NAME}")
                model_path = self.config.MODEL_NAME
                local_files_only = False
            
            # Load processor with error handling
            try:
                logging.info("Loading processor...")
                self.processor = AutoProcessor.from_pretrained(
                    model_path,
                    local_files_only=local_files_only,
                    trust_remote_code=True
                )
                logging.info("‚úÖ Processor loaded successfully")
            except Exception as e:
                logging.error(f"Failed to load processor: {e}")
                if "token" in str(e).lower() or "access" in str(e).lower():
                    logging.error("‚ùå Access denied. Please ensure you have:")
                    logging.error("1. Accepted the Gemma license at: https://huggingface.co/google/gemma-3n-e4b-it")
                    logging.error("2. Valid HuggingFace token (if using private model)")
                raise
            
            # Load model with optimizations
            try:
                logging.info("Loading model...")
                self.model = Gemma3nForConditionalGeneration.from_pretrained(
                    model_path,
                    device_map="auto",
                    torch_dtype=self.config.TORCH_DTYPE,
                    local_files_only=local_files_only,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                ).eval()
                logging.info("‚úÖ Model loaded successfully")
            except Exception as e:
                logging.error(f"Failed to load model: {e}")
                if "memory" in str(e).lower() or "cuda" in str(e).lower():
                    logging.error("‚ùå GPU memory issue. Try:")
                    logging.error("1. Closing other applications")
                    logging.error("2. Reducing batch size")
                    logging.error("3. Using CPU instead of GPU")
                raise
            
            self.model_loaded = True
            
            # Memory optimization and monitoring
            if self.config.DEVICE == "cuda" and torch.cuda.is_available():
                torch.cuda.empty_cache()
                allocated_memory = torch.cuda.memory_allocated(0) / 1024**3
                total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                logging.info(f"GPU Memory: {allocated_memory:.2f}GB / {total_memory:.1f}GB")
                
                # Warn if memory usage is very high
                if allocated_memory > total_memory * 0.9:
                    logging.warning("‚ö†Ô∏è High GPU memory usage detected")
            
            logging.info("üéâ Gemma 3n-E4B-it model loaded successfully!")
            
        except Exception as e:
            logging.error(f"‚ùå Critical error loading model: {e}")
            self.model_loaded = False
            raise
    
    def create_chat_messages(self, text_prompt: str, image: Optional[Image.Image] = None) -> List[Dict]:
        """Create properly formatted chat messages for Gemma 3n"""
        try:
            # Validate inputs
            if not text_prompt or not isinstance(text_prompt, str):
                raise ValueError("Invalid text prompt provided")
            
            messages = [
                {
                    "role": "system",
                    "content": [{"type": "text", "text": "You are a helpful AI assistant specialized in analyzing and understanding images."}]
                }
            ]
            
            user_content = []
            
            # Add image if provided
            if image is not None:
                # Validate image
                if not isinstance(image, Image.Image):
                    logging.warning("Invalid image object provided")
                else:
                    user_content.append({"type": "image", "image": image})
            
            # Add text prompt
            user_content.append({"type": "text", "text": text_prompt})
            
            messages.append({
                "role": "user",
                "content": user_content
            })
            
            return messages
            
        except Exception as e:
            logging.error(f"Error creating chat messages: {e}")
            return []
    
    def analyze_image(self, image: Image.Image, prompt: str = "Describe this image in detail.") -> str:
        """Analyze image with comprehensive error handling"""
        if not self.model_loaded:
            return "‚ùå Error: Model not loaded properly"
        
        try:
            # Validate inputs
            if image is None:
                return "‚ùå Error: No image provided"
            
            if not isinstance(image, Image.Image):
                return "‚ùå Error: Invalid image format"
            
            if not prompt or not isinstance(prompt, str):
                prompt = "Describe this image in detail."
            
            # Create chat messages
            messages = self.create_chat_messages(prompt, image)
            if not messages:
                return "‚ùå Error: Failed to create chat messages"
            
            # Process with the model
            try:
                # Apply chat template
                inputs = self.processor.apply_chat_template(
                    messages,
                    add_generation_prompt=True,
                    tokenize=True,
                    return_dict=True,
                    return_tensors="pt",
                )
                
                # Move to device
                if hasattr(inputs, 'to'):
                    inputs = inputs.to(self.model.device)
                else:
                    # Handle dict inputs
                    inputs = {k: v.to(self.model.device) if hasattr(v, 'to') else v for k, v in inputs.items()}
                
                input_len = inputs["input_ids"].shape[-1]
                
                # Generate with validated parameters
                with torch.inference_mode():
                    generation = self.model.generate(
                        **inputs,
                        max_new_tokens=self.config.MAX_NEW_TOKENS,
                        temperature=self.config.TEMPERATURE,
                        do_sample=self.config.DO_SAMPLE,
                        top_p=self.config.TOP_P,
                        top_k=self.config.TOP_K,
                        repetition_penalty=self.config.REPETITION_PENALTY,
                        pad_token_id=self.processor.tokenizer.eos_token_id,
                        eos_token_id=self.processor.tokenizer.eos_token_id
                    )
                
                # Decode response
                if generation.dim() > 1:
                    generation = generation[0][input_len:]
                else:
                    generation = generation[input_len:]
                
                decoded = self.processor.decode(generation, skip_special_tokens=True)
                
                # Clean up response
                response = decoded.strip()
                if not response:
                    return "No response generated. Please try a different prompt."
                
                return response
                
            except torch.cuda.OutOfMemoryError:
                torch.cuda.empty_cache()
                return "‚ùå Error: GPU out of memory. Try reducing image size or using a shorter prompt."
            except Exception as e:
                logging.error(f"Error during model inference: {e}")
                return f"‚ùå Error during analysis: {str(e)}"
                
        except Exception as e:
            logging.error(f"Error analyzing image: {e}")
            return f"‚ùå Error analyzing image: {str(e)}"
    
    def filter_images_by_query(self, images_with_paths: List[Tuple[str, Image.Image]], 
                              query: str) -> List[Tuple[str, Image.Image, str]]:
        """Filter images with robust error handling and progress tracking"""
        filtered_results = []
        
        if not self.model_loaded:
            logging.error("Model not loaded for filtering")
            return filtered_results
        
        if not images_with_paths:
            logging.warning("No images provided for filtering")
            return filtered_results
        
        # Create robust filtering prompt
        filter_prompt = f"""Look at this image carefully. I am searching for: "{query}"

Respond in exactly this format:
YES or NO
Brief explanation of your decision"""
        
        logging.info(f"Filtering {len(images_with_paths)} images with query: '{query}'")
        
        # Process images with progress tracking
        success_count = 0
        error_count = 0
        
        for i, (image_path, image) in enumerate(tqdm(images_with_paths, desc="Filtering images")):
            try:
                if image is None:
                    error_count += 1
                    continue
                
                response = self.analyze_image(image, filter_prompt)
                
                if response.startswith("‚ùå"):
                    error_count += 1
                    logging.warning(f"Analysis failed for {os.path.basename(image_path)}")
                    continue
                
                # Parse response more robustly
                lines = response.strip().split('\n')
                if len(lines) >= 1:
                    decision_line = lines[0].strip().upper()
                    explanation = '\n'.join(lines[1:]).strip() if len(lines) > 1 else "No explanation provided"
                    
                    # Check for positive match
                    is_match = (
                        decision_line.startswith('YES') or 
                        'yes' in decision_line.lower() or
                        decision_line.startswith('MATCH') or
                        'match' in decision_line.lower() or
                        decision_line.startswith('TRUE') or
                        'found' in decision_line.lower()
                    )
                    
                    if is_match:
                        filtered_results.append((image_path, image, explanation))
                        success_count += 1
                        logging.info(f"‚úÖ Match found: {os.path.basename(image_path)}")
                else:
                    error_count += 1
                    logging.warning(f"Empty response for {os.path.basename(image_path)}")
                    
            except Exception as e:
                error_count += 1
                logging.error(f"Error filtering {os.path.basename(image_path)}: {e}")
                continue
            
            # Periodic memory cleanup
            if i % 3 == 0 and self.config.DEVICE == "cuda":
                torch.cuda.empty_cache()
        
        logging.info(f"üéØ Filtering complete: {len(filtered_results)} matches, {error_count} errors")
        return filtered_results

class VisionLanguageAnalyzer:
    """Main analyzer with comprehensive error handling"""
    
    def __init__(self):
        self.config = Config()
        self.image_processor = ImageProcessor(self.config.DEFAULT_IMAGE_SIZE)
        self.model = Gemma3nVisionModel(self.config)
        self.system_ready = False
        self.error_message = ""
        self.initialize_system()
    
    def initialize_system(self):
        """Initialize system with detailed error tracking"""
        logging.info("üöÄ Initializing Gemma 3n-E4B-it Vision Analysis System...")
        
        try:
            # System validation
            logging.info(f"Device: {self.config.DEVICE}")
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name()
                total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                logging.info(f"GPU: {gpu_name}")
                logging.info(f"VRAM: {total_memory:.1f} GB")
                
                if total_memory < 8.0:
                    logging.warning("‚ö†Ô∏è Low VRAM detected. Performance may be limited.")
            else:
                logging.warning("CUDA not available - using CPU (will be slower)")
            
            # Load model
            self.model.load_model()
            self.system_ready = True
            logging.info("‚úÖ System initialized successfully!")
            
        except Exception as e:
            self.error_message = str(e)
            logging.error(f"‚ùå Failed to initialize system: {e}")
            self.system_ready = False
    
    def get_system_status(self) -> str:
        """Get detailed system status"""
        if self.system_ready:
            return "‚úÖ System Ready - Gemma 3n-E4B-it Loaded"
        else:
            return f"‚ùå System Error: {self.error_message}"
    
    def analyze_single_image(self, image: Image.Image, prompt: str = None) -> str:
        """Analyze single image with validation"""
        if not self.system_ready:
            return f"‚ùå System not ready: {self.error_message}"
        
        try:
            if image is None:
                return "‚ùå No image provided"
            
            if prompt is None or not prompt.strip():
                prompt = "Describe this image in detail, including objects, people, colors, setting, and any notable features you observe."
            
            # Optimize image
            optimized_image = self.image_processor.optimize_image_size(image)
            
            return self.model.analyze_image(optimized_image, prompt)
            
        except Exception as e:
            logging.error(f"Error in single image analysis: {e}")
            return f"‚ùå Error analyzing image: {str(e)}"
    
    def process_image_batch(self, directory_path: str, query: str) -> Tuple[Optional[Image.Image], str]:
        """Process image batch with comprehensive error handling"""
        if not self.system_ready:
            return None, f"‚ùå System not ready: {self.error_message}"
        
        try:
            # Validate inputs
            if not directory_path or not directory_path.strip():
                return None, "‚ùå Please provide a valid directory path"
            
            if not query or not query.strip():
                return None, "‚ùå Please provide a search query"
            
            # Normalize path for Windows
            directory_path = os.path.normpath(directory_path.strip())
            
            # Load images
            images_with_paths = self.image_processor.load_images_from_directory(directory_path)
            
            if not images_with_paths:
                supported_formats = ", ".join(Config.SUPPORTED_FORMATS[:6])  # Show first 6 formats
                return None, f"‚ùå No valid images found in: {directory_path}\n\n**Supported formats:** {supported_formats}\n\n**Troubleshooting:**\n- Check directory path is correct\n- Ensure images are in supported formats\n- Verify file permissions"
            
            # Filter images
            filtered_results = self.model.filter_images_by_query(images_with_paths, query)
            
            if not filtered_results:
                return None, f"‚ùå No images found matching: '{query}'\n\nüìä **Statistics:**\n- **Processed:** {len(images_with_paths)} images\n- **Matches:** 0\n\nüí° **Tips:**\n- Try more general terms (e.g., 'animals' instead of 'golden retriever')\n- Check if images actually contain what you're looking for\n- Verify image quality and clarity\n- Try different query phrasings"
            
            # Generate results
            success_rate = (len(filtered_results) / len(images_with_paths)) * 100
            
            result_text = f"# üéØ Query Results\n\n"
            result_text += f"**Search Query:** \"{query}\"\n"
            result_text += f"**Found:** {len(filtered_results)} matches out of {len(images_with_paths)} total images\n"
            result_text += f"**Success Rate:** {success_rate:.1f}%\n"
            result_text += f"**Model:** Gemma 3n-E4B-it\n"
            result_text += f"**Analysis Time:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
            result_text += "---\n\n"
            result_text += "## üìã Detailed Analysis Results\n\n"
            
            # Process results
            filtered_images = []
            for i, (image_path, image, explanation) in enumerate(filtered_results, 1):
                filtered_images.append(image)
                filename = os.path.basename(image_path)
                result_text += f"### {i}. {filename}\n"
                result_text += f"**üìÅ Location:** `{image_path}`\n"
                result_text += f"**ü§ñ AI Analysis:** {explanation}\n\n"
                result_text += "---\n\n"
            
            # Create image grid
            grid_image = self.image_processor.create_image_grid(filtered_images)
            
            return grid_image, result_text
            
        except Exception as e:
            error_msg = f"‚ùå Error processing batch: {str(e)}"
            logging.error(error_msg)
            return None, error_msg

class GradioInterface:
    """Professional Gradio interface optimized for 5.22.0"""
    
    def __init__(self, analyzer: VisionLanguageAnalyzer):
        self.analyzer = analyzer
        self.config = analyzer.config
    
    def analyze_single_image_interface(self, image, custom_prompt):
        """Single image analysis interface"""
        if image is None:
            return "‚ùå No image uploaded. Please select an image to analyze."
        
        try:
            prompt = custom_prompt if custom_prompt and custom_prompt.strip() else None
            result = self.analyzer.analyze_single_image(image, prompt)
            
            # Format response
            formatted_result = f"# ü§ñ Gemma 3n-E4B-it Analysis\n\n"
            formatted_result += f"**Timestamp:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            formatted_result += f"**Model:** google/gemma-3n-e4b-it\n"
            formatted_result += f"**Image Size:** {image.size[0]}√ó{image.size[1]} pixels\n"
            formatted_result += f"**System Status:** {self.analyzer.get_system_status()}\n\n"
            formatted_result += "---\n\n"
            formatted_result += "### üìù Analysis Result:\n\n"
            formatted_result += result
            
            return formatted_result
            
        except Exception as e:
            return f"‚ùå Error analyzing image: {str(e)}"
    
    def analyze_batch_interface(self, directory_path, query):
        """Batch analysis interface"""
        if not directory_path or not directory_path.strip():
            return None, "‚ùå Please provide a valid directory path containing images."
        
        if not query or not query.strip():
            return None, "‚ùå Please provide a search query to filter images."
        
        try:
            return self.analyzer.process_image_batch(directory_path, query)
        except Exception as e:
            return None, f"‚ùå Error processing batch: {str(e)}"
    
    def create_interface(self) -> gr.Blocks:
        """Create optimized Gradio interface"""
        
        # Custom CSS for better appearance
        custom_css = """
        .gradio-container {
            max-width: 1400px !important;
            margin: auto !important;
        }
        .main-header {
            text-align: center;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 25px;
            border-radius: 15px;
            margin-bottom: 25px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
        .status-box {
            padding: 15px;
            border-radius: 10px;
            margin: 10px 0;
            text-align: center;
            font-weight: bold;
        }
        .status-ready {
            background-color: #d4edda;
            border: 1px solid #c3e6cb;
            color: #155724;
        }
        .status-error {
            background-color: #f8d7da;
            border: 1px solid #f5c6cb;
            color: #721c24;
        }
        .result-box {
            border: 2px solid #e1e5e9;
            border-radius: 10px;
            padding: 20px;
            margin: 15px 0;
            max-height: 600px;
            overflow-y: auto;
            background-color: #f8f9fa;
        }
        .model-info {
            background-color: #e8f4fd;
            border: 1px solid #bee5eb;
            border-radius: 8px;
            padding: 15px;
            margin: 10px 0;
        }
        """
        
        with gr.Blocks(
            theme=gr.themes.Soft(),
            title="Gemma 3n-E4B-it Vision Analysis",
            head="<meta name='viewport' content='width=device-width, initial-scale=1'>",
            css=custom_css
        ) as interface:
            
            # Header
            gr.HTML(f"""
            <div class="main-header">
                <h1>{self.config.INTERFACE_TITLE}</h1>
                <p><strong>Powered by Google's Gemma 3n-E4B-it Model</strong></p>
                <p>4B Effective Parameters ‚Ä¢ MatFormer Architecture ‚Ä¢ RTX A4000 Optimized</p>
            </div>
            """)
            
            # System Status
            status_class = "status-ready" if self.analyzer.system_ready else "status-error"
            status_text = self.analyzer.get_system_status()
            gr.HTML(f'<div class="status-box {status_class}">{status_text}</div>')
            
            # Model information
            gr.HTML("""
            <div class="model-info">
                <h3>üî¨ Model Information</h3>
                <ul>
                    <li><strong>Model:</strong> google/gemma-3n-e4b-it (Instruction-tuned)</li>
                    <li><strong>Architecture:</strong> MatFormer with selective parameter activation</li>
                    <li><strong>Capabilities:</strong> Text + Image ‚Üí Text Generation</li>
                    <li><strong>Context Window:</strong> 32K tokens</li>
                    <li><strong>Languages:</strong> 140+ supported languages</li>
                    <li><strong>Image Support:</strong> 256√ó256, 512√ó512, 768√ó768 pixels</li>
                    <li><strong>Vision Encoder:</strong> MobileNet v5 (768√ó768 default resolution)</li>
                </ul>
            </div>
            """)
            
            # Description
            gr.Markdown(self.config.INTERFACE_DESCRIPTION)
            
            with gr.Tabs():
                # Single Image Analysis Tab
                with gr.Tab("üñºÔ∏è Single Image Analysis"):
                    with gr.Row():
                        with gr.Column(scale=1):
                            gr.Markdown("## üì§ Upload Image")
                            
                            single_image_input = gr.Image(
                                type="pil",
                                label="Select Image for Analysis",
                                height=400,
                                sources=["upload", "clipboard"],
                                show_share_button=False
                            )
                            
                            single_prompt_input = gr.Textbox(
                                label="Custom Analysis Prompt (Optional)",
                                placeholder="e.g., 'What animals are in this image?', 'Describe the emotions and mood'",
                                lines=3,
                                info="Leave empty for default detailed description"
                            )
                            
                            single_analyze_btn = gr.Button(
                                "ü§ñ Analyze with Gemma 3n",
                                variant="primary",
                                size="lg"
                            )
                            
                            # Example prompts
                            gr.Markdown("""
                            ### üí° Example Prompts:
                            - "What is happening in this image?"
                            - "Describe the emotions and mood"
                            - "List all objects you can see"
                            - "What is the setting or location?"
                            - "Are there any people? What are they doing?"
                            - "What colors dominate this image?"
                            - "Is this image taken indoors or outdoors?"
                            - "What story does this image tell?"
                            - "Identify any text or signs in the image"
                            """)
                        
                        with gr.Column(scale=1):
                            gr.Markdown("## üìã Analysis Result")
                            single_result = gr.Markdown(
                                "Upload an image and click 'Analyze with Gemma 3n' to see detailed AI analysis.",
                                elem_classes=["result-box"]
                            )
                
                # Batch Processing Tab
                with gr.Tab("üìÅ Batch Image Processing & Filtering"):
                    with gr.Row():
                        with gr.Column(scale=1):
                            gr.Markdown("## ‚öôÔ∏è Batch Processing Settings")
                            
                            directory_input = gr.Textbox(
                                label="üìÅ Directory Path",
                                placeholder="C:\\Users\\YourName\\Pictures\\MyImages",
                                info="Full path to directory containing images",
                                lines=1
                            )
                            
                            query_input = gr.Textbox(
                                label="üîç Search Query",
                                placeholder="e.g., 'cats', 'people wearing glasses', 'red cars', 'outdoor scenes'",
                                info="Describe what you want to find using natural language",
                                lines=2
                            )
                            
                            batch_analyze_btn = gr.Button(
                                "üöÄ Process Batch with Gemma 3n",
                                variant="primary",
                                size="lg"
                            )
                            
                            gr.Markdown("""
                            ### üéØ Query Examples:
                            - **Animals:** "cats", "dogs", "white cats", "animals sleeping"
                            - **People:** "people smiling", "children playing", "people wearing glasses"
                            - **Objects:** "red cars", "flowers", "food on plates", "books"
                            - **Scenes:** "outdoor scenes", "indoor rooms", "night scenes", "beaches"
                            - **Activities:** "people cooking", "sports activities", "celebrations"
                            - **Colors:** "blue objects", "colorful images", "black and white photos"
                            - **Technical:** "blurry images", "close-up shots", "landscape photos"
                            """)
                            
                            gr.Markdown("""
                            ### ‚ö° Performance Notes:
                            - Processing time: ~4-8 seconds per image
                            - Memory optimized for RTX A4000 (16GB VRAM)
                            - Supports: JPG, PNG, BMP, TIFF, WebP formats
                            - Images automatically resized for optimal processing
                            - Progress tracked in real-time
                            """)
                        
                        with gr.Column(scale=2):
                            gr.Markdown("## üìä Filtered Results")
                            
                            batch_result_images = gr.Image(
                                label="üñºÔ∏è Matching Images (Grid View)",
                                height=400,
                                interactive=False,
                                show_share_button=False,
                                show_download_button=True
                            )
                            
                            batch_result_text = gr.Markdown(
                                "Configure settings and click 'Process Batch' to see filtered results with AI explanations.",
                                elem_classes=["result-box"]
                            )
            
            # Event handlers
            single_analyze_btn.click(
                fn=self.analyze_single_image_interface,
                inputs=[single_image_input, single_prompt_input],
                outputs=[single_result]
            )
            
            batch_analyze_btn.click(
                fn=self.analyze_batch_interface,
                inputs=[directory_input, query_input],
                outputs=[batch_result_images, batch_result_text]
            )
            
            # Footer
            gr.HTML("""
            <div style="text-align: center; margin-top: 30px; padding: 20px; background-color: #f8f9fa; border-radius: 10px; border: 1px solid #dee2e6;">
                <h4>üöÄ Gemma 3n-E4B-it Vision Analysis System v1.2</h4>
                <p><strong>Model:</strong> google/gemma-3n-e4b-it ‚Ä¢ <strong>Hardware:</strong> RTX A4000 Optimized ‚Ä¢ <strong>Mode:</strong> Offline Capable</p>
                <p style="color: #6c757d; font-size: 0.9em;">
                    <em>‚ö†Ô∏è This system provides AI-generated analysis. Results should be verified for critical applications.</em>
                </p>
                <p style="color: #6c757d; font-size: 0.85em;">
                    <strong>Requirements:</strong> transformers ‚â• 4.53.0 ‚Ä¢ PyTorch ‚â• 2.0 ‚Ä¢ CUDA 11.8+ ‚Ä¢ HuggingFace License
                </p>
            </div>
            """)
        
        return interface

def main():
    """Main application entry point with comprehensive error handling"""
    print("üöÄ Initializing Gemma 3n-E4B-it Vision Analysis System...")
    print("=" * 80)
    
    try:
        # System validation
        print("üîß System Requirements Check:")
        print(f"   Python: {sys.version.split()[0]}")
        
        # Check core dependencies
        try:
            print(f"   PyTorch: {torch.__version__}")
            print(f"   CUDA Available: {torch.cuda.is_available()}")
            if torch.cuda.is_available():
                print(f"   GPU: {torch.cuda.get_device_name()}")
                print(f"   CUDA Version: {torch.version.cuda}")
                print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        except Exception as e:
            print(f"   ‚ùå PyTorch issue: {e}")
        
        print(f"   Transformers: {transformers_version}")
        if transformers_version < "4.53.0":
            print("   ‚ùå ERROR: Gemma 3n requires transformers >= 4.53.0")
            print("   Install with: pip install 'transformers>=4.53.0'")
            return
        
        print(f"   Gradio: Available")
        print()
        
        # Initialize analyzer
        analyzer = VisionLanguageAnalyzer()
        
        if not analyzer.system_ready:
            print("‚ùå System initialization failed!")
            print(f"Error: {analyzer.error_message}")
            print("\nüîß Common solutions:")
            print("1. pip install 'transformers>=4.53.0'")
            print("2. Accept Gemma license: https://huggingface.co/google/gemma-3n-e4b-it")
            print("3. Check GPU memory availability")
            print("4. Verify internet connection (for first-time model download)")
            return
        
        # Create and launch interface
        interface_manager = GradioInterface(analyzer)
        app = interface_manager.create_interface()
        
        print("‚úÖ System ready! Starting web interface...")
        print("üåê Access: http://localhost:7860")
        print("üì± Interface will open automatically")
        print("\n" + "=" * 80)
        print("üéØ Features Ready:")
        print("  üì∏ Single Image Analysis - Upload & analyze with custom prompts")
        print("  üìÅ Batch Processing - Filter multiple images with natural language")
        print("  ü§ñ Gemma 3n-E4B-it - Latest Google multimodal AI model")
        print("  ‚ö° RTX A4000 Optimized - 16GB VRAM efficient processing")
        print("  üîß Error Recovery - Comprehensive error handling & recovery")
        print("=" * 80)
        print("Press Ctrl+C to stop")
        
        # Launch with error handling
        app.launch(
            server_name="localhost",
            server_port=7860,
            share=False,
            inbrowser=True,
            show_error=True,
            favicon_path=None,
            ssl_verify=False,
            quiet=False
        )
        
    except KeyboardInterrupt:
        print("\n\nüëã Application stopped by user")
    except Exception as e:
        print(f"\n‚ùå Critical error: {e}")
        print("\nüîß Troubleshooting:")
        print("1. Check all dependencies: pip install -r requirements.txt")
        print("2. Verify CUDA installation and GPU availability") 
        print("3. Ensure sufficient disk space for model files")
        print("4. Check network connection for initial model download")

if __name__ == "__main__":
    # Create required directories with error handling
    try:
        os.makedirs("models", exist_ok=True)
        os.makedirs(os.path.join("models", "gemma-3n-e4b-it"), exist_ok=True)
        os.makedirs("temp", exist_ok=True)
    except Exception as e:
        print(f"Warning: Could not create directories: {e}")
    
    # Run the application
    main()