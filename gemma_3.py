#!/usr/bin/env python3
"""
Professional Multi-Model Vision Analysis Suite - Updated with YOLTv4
=====================================================================

A comprehensive offline vision analysis system with:
1. Gemma 3n-E4B-it for detailed image analysis and description
2. YOLTv4 for state-of-the-art object detection (replacing YOLO12x)
3. YOLOv8x for proven object detection performance

Professional software-grade interface designed for RTX A4000 (16GB VRAM)
with proper GPU memory management to avoid CUDA OOM issues.

Features:
- Multi-model selection with professional UI
- Automatic model offloading to prevent CUDA OOM
- Real-time object detection with proper bounding boxes
- Advanced vision-language analysis 
- Single image and batch processing for all models
- Professional software-style interface
- Local model loading (offline capable)
- GPU optimization for RTX A4000

Author: AI Assistant
Version: 3.1 (YOLTv4 Integration with Memory Management)
"""

import os
import sys
import warnings
import logging
from typing import List, Tuple, Dict, Any, Optional, Union
import json
from datetime import datetime
import glob
from pathlib import Path
import re
import time
import gc

# Core libraries
import numpy as np
import cv2

# PIL for image processing
try:
    from PIL import Image, ImageEnhance, ImageDraw, ImageFont
except ImportError:
    print("Error: Pillow (PIL) is required. Install with: pip install Pillow")
    sys.exit(1)

# PyTorch
try:
    import torch
    if torch.__version__ < "2.0.0":
        print(f"Warning: PyTorch {torch.__version__} detected. Recommended: >= 2.0.0")
except ImportError:
    print("Error: PyTorch is required. Install with: pip install torch")
    sys.exit(1)

# Transformers for Gemma 3n
try:
    from transformers import (
        AutoProcessor, 
        Gemma3nForConditionalGeneration,
        __version__ as transformers_version
    )
    GEMMA_AVAILABLE = transformers_version >= "4.53.0"
except ImportError:
    print("Warning: Transformers not available for Gemma 3n")
    GEMMA_AVAILABLE = False
    transformers_version = "0.0.0"

# Ultralytics YOLO for object detection
try:
    from ultralytics import YOLO
    from ultralytics.utils.plotting import Annotator, colors
    YOLO_AVAILABLE = True
except ImportError:
    print("Warning: Ultralytics YOLO not available. Install with: pip install ultralytics")
    YOLO_AVAILABLE = False

# Gradio for UI
try:
    import gradio as gr
    gradio_version = getattr(gr, '__version__', '0.0.0')
except ImportError:
    print("âŒ Error: Gradio not installed. Please install: pip install gradio")
    sys.exit(1)

# Optional dependencies
try:
    from tqdm import tqdm
except ImportError:
    class tqdm:
        def __init__(self, iterable, desc="", **kwargs):
            self.iterable = iterable
        def __iter__(self):
            return iter(self.iterable)

# Suppress warnings
warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class Config:
    """Professional configuration for all models"""
    
    # Model paths - Updated with YOLTv4
    GEMMA_MODEL_NAME = "google/gemma-3n-e4b-it"
    GEMMA_MODEL_PATH = os.path.join("models", "gemma-3n-e4b-it")
    YOLTV4_MODEL_PATH = os.path.join("models", "yoltv4.pt")  # Updated: YOLTv4 instead of YOLO12x
    YOLOV8X_MODEL_PATH = os.path.join("models", "yolov8x.pt")
    
    # System settings
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    TORCH_DTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float32
    SUPPORTED_FORMATS = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp', '.JPG', '.JPEG', '.PNG', '.BMP', '.TIFF', '.WEBP']
    
    # Gemma 3n settings
    GEMMA_MAX_NEW_TOKENS = 512
    GEMMA_TEMPERATURE = 0.8
    GEMMA_IMAGE_SIZES = [256, 512, 768]
    GEMMA_DEFAULT_SIZE = 512
    
    # YOLO settings (optimized for professional use)
    YOLO_CONFIDENCE = 0.25      # Detection confidence threshold
    YOLO_IOU = 0.45            # IoU threshold for NMS  
    YOLO_MAX_DETECTIONS = 300  # Maximum detections per image
    YOLO_LINE_WIDTH = 3        # Professional bounding box thickness
    YOLO_FONT_SIZE = 14        # Professional label font size
    
    # COCO class names (80 classes)
    COCO_CLASSES = [
        'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',
        'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',
        'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',
        'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
        'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
        'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
        'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',
        'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop',
        'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',
        'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
    ]
    
    # UI settings - Updated with YOLTv4
    INTERFACE_TITLE = "ðŸ”¬ Professional Vision Analysis Suite"
    INTERFACE_DESCRIPTION = """
    ## Enterprise-Grade Multi-Model Vision Analysis Platform
    
    Select from three state-of-the-art AI models for comprehensive vision analysis:
    
    **ðŸ¤– Gemma 3n-E4B-it**: Advanced vision-language model for detailed image understanding and natural language descriptions
    **ðŸŽ¯ YOLTv4**: Latest state-of-the-art object detection with enhanced accuracy and performance 
    **âš¡ YOLOv8x**: Proven high-performance object detection with real-time processing
    
    Professional-grade interface designed for research, development, and production environments.
    """

class ImageProcessor:
    """Professional image processing pipeline"""
    
    def __init__(self):
        self.gemma_sizes = Config.GEMMA_IMAGE_SIZES
        
    def is_valid_image_file(self, file_path: str) -> bool:
        """Validate image file"""
        try:
            _, ext = os.path.splitext(file_path.lower())
            if ext not in [fmt.lower() for fmt in Config.SUPPORTED_FORMATS]:
                return False
            
            if not os.path.isfile(file_path):
                return False
                
            with Image.open(file_path) as img:
                img.verify()
            return True
        except Exception:
            return False
    
    def prepare_image_for_gemma(self, image: Image.Image, target_size: int = 512) -> Image.Image:
        """Prepare image for Gemma 3n processing"""
        try:
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            width, height = image.size
            max_dim = max(width, height)
            
            # Choose optimal size for Gemma 3n
            if max_dim <= self.gemma_sizes[0]:
                target_size = self.gemma_sizes[0]
            elif max_dim <= self.gemma_sizes[1]:
                target_size = self.gemma_sizes[1]
            else:
                target_size = self.gemma_sizes[2]
            
            # Resize maintaining aspect ratio, then pad to square
            if width > height:
                new_width = target_size
                new_height = int((height * target_size) / width)
            else:
                new_height = target_size
                new_width = int((width * target_size) / height)
            
            new_width = max(1, new_width)
            new_height = max(1, new_height)
            
            image = image.resize((new_width, new_height), Image.LANCZOS)
            
            # Create square canvas and center
            canvas = Image.new('RGB', (target_size, target_size), (255, 255, 255))
            x_offset = (target_size - new_width) // 2
            y_offset = (target_size - new_height) // 2
            canvas.paste(image, (max(0, x_offset), max(0, y_offset)))
            
            return canvas
        except Exception as e:
            logging.error(f"Error preparing image for Gemma: {e}")
            return image
    
    def prepare_image_for_yolo(self, image: Image.Image) -> np.ndarray:
        """Prepare image for YOLO processing (returns numpy array in RGB format)"""
        try:
            if image.mode != 'RGB':
                image = image.convert('RGB')
            # Convert PIL to numpy array in RGB format
            return np.array(image)
        except Exception as e:
            logging.error(f"Error preparing image for YOLO: {e}")
            return np.array(image)
    
    def load_image(self, image_path: str) -> Optional[Image.Image]:
        """Load and validate image"""
        try:
            if not self.is_valid_image_file(image_path):
                return None
            
            with Image.open(image_path) as img:
                return img.copy().convert('RGB')
        except Exception as e:
            logging.error(f"Error loading image {image_path}: {e}")
            return None
    
    def load_images_from_directory(self, directory_path: str) -> List[Tuple[str, Image.Image]]:
        """Load all images from directory"""
        images = []
        try:
            directory = Path(directory_path)
            if not directory.exists() or not directory.is_dir():
                logging.error(f"Invalid directory: {directory_path}")
                return images
            
            # Find all image files
            for ext in Config.SUPPORTED_FORMATS:
                for image_path in directory.glob(f"*{ext}"):
                    image = self.load_image(str(image_path))
                    if image:
                        images.append((str(image_path), image))
            
            logging.info(f"Loaded {len(images)} images from {directory_path}")
            return images
        except Exception as e:
            logging.error(f"Error loading images from directory: {e}")
            return images
    
    def create_image_grid(self, images: List[Image.Image], max_cols: int = 4) -> Optional[Image.Image]:
        """Create professional image grid"""
        try:
            if not images:
                return None
            
            valid_images = [img for img in images if img is not None]
            if not valid_images:
                return None
            
            num_images = len(valid_images)
            cols = min(max_cols, num_images)
            rows = (num_images + cols - 1) // cols
            
            cell_size = 250  # Larger for professional display
            padding = 10
            
            grid_width = cols * cell_size + (cols - 1) * padding
            grid_height = rows * cell_size + (rows - 1) * padding
            grid_image = Image.new('RGB', (grid_width, grid_height), (248, 249, 250))
            
            for idx, image in enumerate(valid_images):
                try:
                    row = idx // cols
                    col = idx % cols
                    x = col * (cell_size + padding)
                    y = row * (cell_size + padding)
                    
                    if image.size[0] > 0 and image.size[1] > 0:
                        resized_image = image.resize((cell_size, cell_size), Image.LANCZOS)
                        grid_image.paste(resized_image, (x, y))
                except Exception as e:
                    logging.warning(f"Error placing image {idx}: {e}")
                    continue
            
            return grid_image
        except Exception as e:
            logging.error(f"Error creating image grid: {e}")
            return None

class GemmaModel:
    """Gemma 3n-E4B-it model with professional implementation and memory management"""
    
    def __init__(self, config: Config):
        self.config = config
        self.model = None
        self.processor = None
        self.loaded = False
        
    def offload_model(self):
        """Offload Gemma model from GPU to free memory"""
        try:
            if self.model is not None:
                self.model.to('cpu')
                del self.model
                self.model = None
            if self.processor is not None:
                del self.processor
                self.processor = None
            self.loaded = False
            
            # Clear GPU cache
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            
            logging.info("âœ… Gemma model offloaded from GPU")
        except Exception as e:
            logging.warning(f"Error offloading Gemma model: {e}")
        
    def load_model(self):
        """Load Gemma 3n model professionally"""
        if not GEMMA_AVAILABLE:
            raise ValueError("Gemma 3n requires transformers >= 4.53.0")
        
        logging.info("Loading Gemma 3n-E4B-it model...")
        
        try:
            # Determine model path
            model_path = self.config.GEMMA_MODEL_PATH
            if os.path.exists(model_path) and os.path.isdir(model_path):
                logging.info(f"Loading from local: {model_path}")
                local_files_only = True
            else:
                logging.info(f"Loading from online: {self.config.GEMMA_MODEL_NAME}")
                model_path = self.config.GEMMA_MODEL_NAME
                local_files_only = False
            
            # Load processor
            self.processor = AutoProcessor.from_pretrained(
                model_path,
                local_files_only=local_files_only,
                trust_remote_code=True
            )
            
            # Load model with proper device management
            self.model = Gemma3nForConditionalGeneration.from_pretrained(
                model_path,
                torch_dtype=self.config.TORCH_DTYPE,
                local_files_only=local_files_only,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            
            # Properly move to device
            self.model = self.model.to(self.config.DEVICE)
            self.model.eval()
            
            self.loaded = True
            logging.info("âœ… Gemma 3n model loaded successfully")
            
        except Exception as e:
            logging.error(f"Failed to load Gemma 3n: {e}")
            raise
    
    def analyze_image(self, image: Image.Image, prompt: str = None) -> str:
        """Professional image analysis with Gemma 3n"""
        if not self.loaded:
            return "âŒ Gemma 3n model not loaded"
        
        try:
            if prompt is None:
                prompt = "Provide a comprehensive and professional analysis of this image, covering: 1) All objects, people, and elements present, 2) Setting, environment, and context, 3) Colors, lighting, and mood, 4) Composition and artistic elements, 5) Any activities or interactions, 6) Technical aspects and quality, 7) Overall narrative and significance. Be detailed and thorough."
            
            # Create professional chat messages
            messages = [
                {
                    "role": "system",
                    "content": [{"type": "text", "text": "You are a professional image analyst providing comprehensive, detailed descriptions for technical documentation and analysis reports."}]
                },
                {
                    "role": "user", 
                    "content": [
                        {"type": "image", "image": image},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
            
            # Process with model
            inputs = self.processor.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_dict=True,
                return_tensors="pt",
            )
            
            if hasattr(inputs, 'to'):
                inputs = inputs.to(self.model.device)
            else:
                inputs = {k: v.to(self.model.device) if hasattr(v, 'to') else v for k, v in inputs.items()}
            
            input_len = inputs["input_ids"].shape[-1]
            
            # Generate professional response
            with torch.inference_mode():
                generation = self.model.generate(
                    **inputs,
                    max_new_tokens=self.config.GEMMA_MAX_NEW_TOKENS,
                    temperature=self.config.GEMMA_TEMPERATURE,
                    do_sample=True,
                    top_p=0.9,
                    top_k=40,
                    repetition_penalty=1.1,
                    pad_token_id=self.processor.tokenizer.eos_token_id,
                    eos_token_id=self.processor.tokenizer.eos_token_id
                )
            
            # Decode response
            if generation.dim() > 1:
                generation = generation[0][input_len:]
            else:
                generation = generation[input_len:]
            
            response = self.processor.decode(generation, skip_special_tokens=True).strip()
            return response if response else "No response generated. Please try a different prompt."
            
        except torch.cuda.OutOfMemoryError:
            torch.cuda.empty_cache()
            return "âŒ GPU out of memory. Try reducing image size or prompt length."
        except Exception as e:
            logging.error(f"Gemma analysis error: {e}")
            return f"âŒ Analysis error: {str(e)}"

class YOLOModel:
    """Professional YOLO model implementation with proper visualization and memory management"""
    
    def __init__(self, config: Config, model_type: str):
        self.config = config
        self.model_type = model_type  # "YOLTv4" or "YOLOv8x"
        self.model = None
        self.loaded = False
        
        # Professional color palette for bounding boxes
        self.colors = [
            (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255),
            (0, 255, 255), (128, 0, 0), (0, 128, 0), (0, 0, 128), (128, 128, 0),
            (128, 0, 128), (0, 128, 128), (255, 128, 0), (255, 0, 128), (128, 255, 0),
            (0, 255, 128), (128, 0, 255), (0, 128, 255), (255, 128, 128), (128, 255, 128)
        ]
        
    def offload_model(self):
        """Offload YOLO model from GPU to free memory"""
        try:
            if self.model is not None:
                # Move model to CPU if possible
                if hasattr(self.model, 'model') and hasattr(self.model.model, 'to'):
                    self.model.model.to('cpu')
                del self.model
                self.model = None
            self.loaded = False
            
            # Clear GPU cache
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            
            logging.info(f"âœ… {self.model_type} model offloaded from GPU")
        except Exception as e:
            logging.warning(f"Error offloading {self.model_type} model: {e}")
        
    def load_model(self):
        """Load YOLO model professionally"""
        if not YOLO_AVAILABLE:
            raise ValueError("Ultralytics YOLO not available. Install with: pip install ultralytics")
        
        logging.info(f"Loading {self.model_type} model...")
        
        try:
            if self.model_type == "YOLTv4":  # Updated: YOLTv4 instead of YOLO12x
                model_path = self.config.YOLTV4_MODEL_PATH
                fallback_model = "yolo11n.pt"  # Using YOLO11n as fallback for YOLTv4
            else:  # YOLOv8x
                model_path = self.config.YOLOV8X_MODEL_PATH
                fallback_model = "yolov8x.pt"
            
            if os.path.exists(model_path):
                logging.info(f"Loading {self.model_type} from local: {model_path}")
                self.model = YOLO(model_path)
            else:
                logging.info(f"Local {self.model_type} not found, downloading {fallback_model}...")
                self.model = YOLO(fallback_model)
                logging.info(f"Note: Using {fallback_model} as fallback. Place {self.model_type.lower()}.pt in models/ for the specific model")
            
            # Configure model for professional use
            self.model.conf = self.config.YOLO_CONFIDENCE
            self.model.iou = self.config.YOLO_IOU
            self.model.max_det = self.config.YOLO_MAX_DETECTIONS
            
            self.loaded = True
            logging.info(f"âœ… {self.model_type} model loaded successfully")
            
        except Exception as e:
            logging.error(f"Failed to load {self.model_type}: {e}")
            raise
    
    def detect_objects(self, image_array: np.ndarray) -> Tuple[Image.Image, str]:
        """Professional object detection with proper bounding boxes"""
        if not self.loaded:
            return Image.fromarray(image_array), f"âŒ {self.model_type} model not loaded"
        
        try:
            # Run detection on RGB image array
            results = self.model.predict(
                source=image_array,
                conf=self.config.YOLO_CONFIDENCE,
                iou=self.config.YOLO_IOU,
                max_det=self.config.YOLO_MAX_DETECTIONS,
                verbose=False
            )
            
            # Process results
            if len(results) == 0 or len(results[0].boxes) == 0:
                return Image.fromarray(image_array), "No objects detected in the image."
            
            # Get the first result (single image)
            result = results[0]
            
            # Create annotated image using Ultralytics' professional annotator
            annotated_array = result.plot(
                conf=True,  # Show confidence scores
                labels=True,  # Show labels
                boxes=True,  # Show bounding boxes
                line_width=self.config.YOLO_LINE_WIDTH,
                font_size=self.config.YOLO_FONT_SIZE
            )
            
            # Convert BGR to RGB (Ultralytics returns BGR)
            annotated_array = cv2.cvtColor(annotated_array, cv2.COLOR_BGR2RGB)
            annotated_image = Image.fromarray(annotated_array)
            
            # Create professional detection summary
            summary = self._create_professional_summary(result)
            
            return annotated_image, summary
            
        except Exception as e:
            logging.error(f"{self.model_type} detection error: {e}")
            return Image.fromarray(image_array), f"âŒ Detection error: {str(e)}"
    
    def _create_professional_summary(self, result) -> str:
        """Create professional detection summary"""
        boxes = result.boxes
        
        if boxes is None or len(boxes) == 0:
            return "No objects detected."
        
        # Extract detection data
        detections = []
        class_counts = {}
        
        for box in boxes:
            # Get box data
            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
            confidence = float(box.conf[0].cpu().numpy())
            class_id = int(box.cls[0].cpu().numpy())
            
            # Get class name
            if hasattr(result, 'names') and class_id in result.names:
                class_name = result.names[class_id]
            elif class_id < len(Config.COCO_CLASSES):
                class_name = Config.COCO_CLASSES[class_id]
            else:
                class_name = f"Class_{class_id}"
            
            detections.append({
                'class': class_name,
                'confidence': confidence,
                'bbox': [float(x1), float(y1), float(x2), float(y2)]
            })
            
            # Count classes
            if class_name not in class_counts:
                class_counts[class_name] = []
            class_counts[class_name].append(confidence)
        
        # Create professional summary
        summary = f"# ðŸŽ¯ {self.model_type} Object Detection Analysis\n\n"
        summary += f"**Model:** {self.model_type} Professional Object Detection\n"
        summary += f"**Total Objects Detected:** {len(detections)}\n"
        summary += f"**Confidence Threshold:** {self.config.YOLO_CONFIDENCE:.2f}\n"
        summary += f"**Analysis Timestamp:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        
        summary += "---\n\n## ðŸ“Š Detection Summary by Class:\n\n"
        
        # Sort by number of detections
        sorted_classes = sorted(class_counts.items(), key=lambda x: len(x[1]), reverse=True)
        
        for class_name, confidences in sorted_classes:
            count = len(confidences)
            avg_conf = sum(confidences) / count
            max_conf = max(confidences)
            min_conf = min(confidences)
            
            summary += f"### {class_name.title()}\n"
            summary += f"- **Count:** {count} object{'s' if count > 1 else ''}\n"
            summary += f"- **Average Confidence:** {avg_conf:.3f}\n"
            summary += f"- **Confidence Range:** {min_conf:.3f} - {max_conf:.3f}\n\n"
        
        summary += "---\n\n## ðŸ” Technical Details:\n\n"
        summary += f"- **Detection Model:** {self.model_type} (COCO-trained, 80 classes)\n"
        summary += f"- **IoU Threshold:** {self.config.YOLO_IOU}\n"
        summary += f"- **Maximum Detections:** {self.config.YOLO_MAX_DETECTIONS}\n"
        summary += f"- **Processing Mode:** Professional Annotation\n\n"
        
        summary += "**Visualization:** Each detected object is highlighted with:\n"
        summary += "- Colored bounding box for precise localization\n"
        summary += "- Class label with confidence score\n"
        summary += "- Professional-grade annotation styling\n\n"
        
        # Performance assessment
        high_conf_detections = [d for d in detections if d['confidence'] >= 0.7]
        if len(high_conf_detections) == len(detections):
            summary += "**Quality Assessment:** âœ… All detections have high confidence (â‰¥0.7)\n"
        elif len(high_conf_detections) >= len(detections) * 0.8:
            summary += "**Quality Assessment:** âœ… Most detections have high confidence\n"
        else:
            summary += "**Quality Assessment:** âš ï¸ Some detections have lower confidence\n"
        
        return summary

class MultiModelAnalyzer:
    """Professional multi-model analyzer with improved memory management"""
    
    def __init__(self):
        self.config = Config()
        self.image_processor = ImageProcessor()
        self.gemma_model = GemmaModel(self.config) if GEMMA_AVAILABLE else None
        self.yoltv4_model = YOLOModel(self.config, "YOLTv4") if YOLO_AVAILABLE else None  # Updated: YOLTv4
        self.yolov8x_model = YOLOModel(self.config, "YOLOv8x") if YOLO_AVAILABLE else None
        self.current_model = None
        self.current_model_type = None
        
    def get_available_models(self) -> List[str]:
        """Get list of available models"""
        models = []
        if self.gemma_model is not None:
            models.append("Gemma 3n-E4B-it")
        if self.yoltv4_model is not None:
            models.append("YOLTv4")  # Updated: YOLTv4
        if self.yolov8x_model is not None:
            models.append("YOLOv8x")
        return models
    
    def offload_current_model(self):
        """Offload current model to free GPU memory"""
        try:
            if self.current_model is not None:
                if hasattr(self.current_model, 'offload_model'):
                    self.current_model.offload_model()
                
                self.current_model = None
                self.current_model_type = None
                
                # Additional cleanup
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc.collect()
                
                logging.info("âœ… Current model offloaded and memory freed")
        except Exception as e:
            logging.warning(f"Error during model offloading: {e}")
    
    def load_model(self, model_type: str) -> str:
        """Load selected model professionally with memory management"""
        try:
            # First, offload current model to free memory
            self.offload_current_model()
            
            if model_type == "Gemma 3n-E4B-it":
                if self.gemma_model is None:
                    return "âŒ Gemma 3n not available. Install transformers >= 4.53.0"
                if not self.gemma_model.loaded:
                    self.gemma_model.load_model()
                self.current_model = self.gemma_model
                self.current_model_type = "gemma"
                return "âœ… Gemma 3n-E4B-it loaded successfully - Ready for advanced image analysis"
                
            elif model_type == "YOLTv4":  # Updated: YOLTv4
                if self.yoltv4_model is None:
                    return "âŒ YOLTv4 not available. Install ultralytics"
                if not self.yoltv4_model.loaded:
                    self.yoltv4_model.load_model()
                self.current_model = self.yoltv4_model
                self.current_model_type = "yolo"
                return "âœ… YOLTv4 loaded successfully - Ready for state-of-the-art object detection"
                
            elif model_type == "YOLOv8x":
                if self.yolov8x_model is None:
                    return "âŒ YOLOv8x not available. Install ultralytics"
                if not self.yolov8x_model.loaded:
                    self.yolov8x_model.load_model()
                self.current_model = self.yolov8x_model
                self.current_model_type = "yolo"
                return "âœ… YOLOv8x loaded successfully - Ready for proven object detection"
                
            else:
                return f"âŒ Unknown model type: {model_type}"
                
        except Exception as e:
            return f"âŒ Error loading {model_type}: {str(e)}"
    
    def process_single_image(self, image: Image.Image, prompt_or_settings: str = None) -> Union[str, Tuple[Image.Image, str]]:
        """Process single image with current model"""
        if self.current_model is None:
            if self.current_model_type == "yolo":
                return image, "âŒ No model loaded. Please select and load a model first."
            else:
                return "âŒ No model loaded. Please select and load a model first."
        
        try:
            if self.current_model_type == "gemma":
                # Process with Gemma 3n
                processed_image = self.image_processor.prepare_image_for_gemma(image)
                result = self.current_model.analyze_image(processed_image, prompt_or_settings)
                return result
                
            elif self.current_model_type == "yolo":
                # Process with YOLO
                image_array = self.image_processor.prepare_image_for_yolo(image)
                annotated_image, summary = self.current_model.detect_objects(image_array)
                return annotated_image, summary
                
        except Exception as e:
            logging.error(f"Error processing image: {e}")
            if self.current_model_type == "yolo":
                return image, f"âŒ Processing error: {str(e)}"
            else:
                return f"âŒ Processing error: {str(e)}"
    
    def process_image_batch(self, directory_path: str, query_or_settings: str = None) -> Tuple[Optional[Image.Image], str]:
        """Process batch of images professionally"""
        if self.current_model is None:
            return None, "âŒ No model loaded. Please select and load a model first."
        
        try:
            # Load images
            images_with_paths = self.image_processor.load_images_from_directory(directory_path)
            
            if not images_with_paths:
                return None, f"âŒ No valid images found in: {directory_path}"
            
            if self.current_model_type == "gemma":
                return self._process_gemma_batch(images_with_paths, query_or_settings)
            elif self.current_model_type == "yolo":
                return self._process_yolo_batch(images_with_paths)
                
        except Exception as e:
            return None, f"âŒ Batch processing error: {str(e)}"
    
    def _process_gemma_batch(self, images_with_paths: List[Tuple[str, Image.Image]], query: str) -> Tuple[Optional[Image.Image], str]:
        """Process batch with Gemma 3n"""
        if not query:
            return None, "âŒ Please provide a search query for batch filtering"
        
        filter_prompt = f"""Analyze this image carefully. I am searching for images that contain: "{query}". Respond in exactly this format:
MATCH or NO_MATCH
Brief explanation of your decision (1-2 sentences)"""
        
        filtered_results = []
        processing_stats = {"total": len(images_with_paths), "processed": 0, "matches": 0, "errors": 0}
        
        for image_path, image in tqdm(images_with_paths, desc="Analyzing images with Gemma 3n"):
            try:
                processed_image = self.image_processor.prepare_image_for_gemma(image)
                response = self.current_model.analyze_image(processed_image, filter_prompt)
                
                processing_stats["processed"] += 1
                
                lines = response.strip().split('\n')
                if lines and ('MATCH' in lines[0].upper() or 'YES' in lines[0].upper()):
                    explanation = '\n'.join(lines[1:]).strip() if len(lines) > 1 else "Match found"
                    filtered_results.append((image_path, image, explanation))
                    processing_stats["matches"] += 1
                    
            except Exception as e:
                logging.warning(f"Error processing {image_path}: {e}")
                processing_stats["errors"] += 1
                continue
            
            # Memory management
            if processing_stats["processed"] % 5 == 0:
                torch.cuda.empty_cache()
        
        if not filtered_results:
            return None, f"âŒ No images found matching: '{query}'\n\nðŸ“Š **Processing Statistics:**\n- Total images: {processing_stats['total']}\n- Successfully processed: {processing_stats['processed']}\n- Errors: {processing_stats['errors']}"
        
        # Create professional results
        filtered_images = [img for _, img, _ in filtered_results]
        grid_image = self.image_processor.create_image_grid(filtered_images)
        
        # Create professional summary
        success_rate = (processing_stats["matches"] / processing_stats["processed"]) * 100 if processing_stats["processed"] > 0 else 0
        
        result_text = f"# ðŸ” Professional Batch Analysis Results\n\n"
        result_text += f"**Analysis Model:** Gemma 3n-E4B-it\n"
        result_text += f"**Search Query:** \"{query}\"\n"
        result_text += f"**Analysis Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        
        result_text += "## ðŸ“Š Processing Statistics:\n\n"
        result_text += f"- **Total Images:** {processing_stats['total']}\n"
        result_text += f"- **Successfully Processed:** {processing_stats['processed']}\n"
        result_text += f"- **Matching Images:** {processing_stats['matches']}\n"
        result_text += f"- **Success Rate:** {success_rate:.1f}%\n"
        result_text += f"- **Processing Errors:** {processing_stats['errors']}\n\n"
        
        result_text += "---\n\n## ðŸŽ¯ Matched Images Analysis:\n\n"
        
        for i, (path, _, explanation) in enumerate(filtered_results, 1):
            filename = os.path.basename(path)
            result_text += f"### {i}. {filename}\n"
            result_text += f"**Location:** `{path}`\n"
            result_text += f"**AI Analysis:** {explanation}\n\n"
        
        return grid_image, result_text
    
    def _process_yolo_batch(self, images_with_paths: List[Tuple[str, Image.Image]]) -> Tuple[List[Tuple[Image.Image, str]], str]:
        """Process batch with YOLO models"""
        processed_images_with_info = []
        detection_stats = {"total": len(images_with_paths), "processed": 0, "total_objects": 0, "class_counts": {}}
        
        for image_path, image in tqdm(images_with_paths, desc=f"Processing with {self.current_model.model_type}"):
            try:
                image_array = self.image_processor.prepare_image_for_yolo(image)
                annotated_image, summary = self.current_model.detect_objects(image_array)
                
                # Create caption with filename and detection info
                filename = os.path.basename(image_path)
                
                # Extract object count from summary
                object_count = 0
                if "Total Objects Detected:" in summary:
                    try:
                        count_line = [line for line in summary.split('\n') if 'Total Objects Detected:' in line][0]
                        object_count = int(count_line.split(':')[1].strip())
                        detection_stats["total_objects"] += object_count
                    except:
                        pass
                
                # Create image caption
                caption = f"{filename} | {object_count} objects detected"
                
                processed_images_with_info.append((annotated_image, caption))
                detection_stats["processed"] += 1
                    
            except Exception as e:
                logging.warning(f"Error processing {image_path}: {e}")
                # Add original image with error caption
                filename = os.path.basename(image_path)
                processed_images_with_info.append((image, f"{filename} | Processing Error"))
                continue
        
        # Create professional batch summary
        avg_objects = detection_stats["total_objects"] / detection_stats["processed"] if detection_stats["processed"] > 0 else 0
        
        result_text = f"# ðŸŽ¯ Professional Batch Object Detection\n\n"
        result_text += f"**Detection Model:** {self.current_model.model_type}\n"
        result_text += f"**Analysis Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        result_text += f"**Confidence Threshold:** {self.config.YOLO_CONFIDENCE}\n\n"
        
        result_text += "## ðŸ“Š Batch Processing Statistics:\n\n"
        result_text += f"- **Total Images:** {detection_stats['total']}\n"
        result_text += f"- **Successfully Processed:** {detection_stats['processed']}\n"
        result_text += f"- **Total Objects Detected:** {detection_stats['total_objects']}\n"
        result_text += f"- **Average Objects per Image:** {avg_objects:.1f}\n\n"
        
        result_text += "## ðŸ” Professional Analysis Summary:\n\n"
        result_text += "Each image below has been processed with state-of-the-art object detection, showing:\n\n"
        result_text += "- **Precise Bounding Boxes:** Accurate object localization\n"
        result_text += "- **Confidence Scores:** Reliability indicators for each detection\n"
        result_text += "- **Class Labels:** Identification from 80 COCO object categories\n"
        result_text += "- **Professional Annotation:** Publication-ready visualizations\n\n"
        
        result_text += f"**Note:** All images processed using {self.current_model.model_type} professional object detection model with optimized parameters for accuracy and performance.\n"
        
        return processed_images_with_info, result_text

class ProfessionalGradioInterface:
    """Professional software-grade Gradio interface with YOLTv4 integration"""
    
    def __init__(self, analyzer: MultiModelAnalyzer):
        self.analyzer = analyzer
        self.config = analyzer.config
        
    def load_model_interface(self, model_choice):
        """Professional model loading interface"""
        if not model_choice:
            return "âŒ Please select a model from the dropdown", gr.update(), gr.update(), gr.update()
        
        # Load the model
        result = self.analyzer.load_model(model_choice)
        
        # Update interface visibility based on model type
        if "âœ…" in result:
            if model_choice == "Gemma 3n-E4B-it":
                return result, gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)
            elif model_choice == "YOLTv4":  # Updated: YOLTv4
                return result, gr.update(visible=False), gr.update(visible=True), gr.update(visible=False)
            else:  # YOLOv8x
                return result, gr.update(visible=False), gr.update(visible=False), gr.update(visible=True)
        else:
            return result, gr.update(visible=False), gr.update(visible=False), gr.update(visible=False)
    
    def process_single_gemma(self, image, prompt):
        """Process single image with Gemma 3n"""
        if image is None:
            return "âŒ Please upload an image for analysis"
        
        start_time = time.time()
        result = self.analyzer.process_single_image(image, prompt)
        processing_time = time.time() - start_time
        
        # Format professional result
        formatted = f"# ðŸ¤– Gemma 3n-E4B-it Professional Analysis\n\n"
        formatted += f"**Analysis Timestamp:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        formatted += f"**Model:** google/gemma-3n-e4b-it (4B effective parameters)\n"
        formatted += f"**Image Specifications:** {image.size[0]}Ã—{image.size[1]} pixels\n"
        formatted += f"**Processing Time:** {processing_time:.2f} seconds\n"
        formatted += f"**Analysis Length:** {len(result.split())} words\n\n"
        formatted += "---\n\n### ðŸ“‹ Comprehensive Analysis Report:\n\n"
        formatted += result
        
        return formatted
    
    def process_single_yolo(self, image, model_type):
        """Process single image with YOLO models"""
        if image is None:
            return None, f"âŒ Please upload an image for {model_type} object detection"
        
        start_time = time.time()
        result = self.analyzer.process_single_image(image)
        processing_time = time.time() - start_time
        
        if isinstance(result, tuple):
            annotated_image, summary = result
            
            # Add processing info to summary
            enhanced_summary = summary.replace(
                f"**Analysis Timestamp:**",
                f"**Processing Time:** {processing_time:.2f} seconds\n**Analysis Timestamp:**"
            )
            
            return annotated_image, enhanced_summary
        else:
            return None, result
    
    def process_batch_gemma(self, directory, query):
        """Process batch with Gemma 3n"""
        if not directory or not query:
            return None, "âŒ Please provide both directory path and search query"
        
        return self.analyzer.process_image_batch(directory, query)
    
    def process_batch_yolo(self, directory, model_type):
        """Process batch with YOLO models"""
        if not directory:
            return None, f"âŒ Please provide directory path for {model_type} batch processing"
        
        return self.analyzer.process_image_batch(directory)
    
    def create_interface(self) -> gr.Blocks:
        """Create professional software-grade interface with YOLTv4"""
        
        # Professional software CSS
        professional_css = """
        .gradio-container {
            max-width: 1800px !important;
            margin: auto !important;
            font-family: 'Segoe UI', -apple-system, BlinkMacSystemFont, 'Roboto', sans-serif;
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        }
        
        .main-header {
            text-align: center;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 50%, #f093fb 100%);
            color: white;
            padding: 40px 30px;
            border-radius: 20px;
            margin-bottom: 30px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
            border: 3px solid rgba(255, 255, 255, 0.2);
        }
        
        .model-selector-card {
            background: linear-gradient(135deg, #ffffff 0%, #f8f9fa 100%);
            border: 3px solid #e9ecef;
            border-radius: 20px;
            padding: 30px;
            margin: 25px 0;
            box-shadow: 0 15px 35px rgba(0, 0, 0, 0.1);
        }
        
        .status-display {
            padding: 20px;
            border-radius: 15px;
            margin: 20px 0;
            text-align: center;
            font-weight: bold;
            font-size: 16px;
            border: 2px solid;
        }
        
        .model-info-card {
            background: linear-gradient(135deg, #e8f4fd 0%, #d1ecf1 100%);
            border: 2px solid #3498db;
            border-radius: 15px;
            padding: 25px;
            margin: 20px 0;
            box-shadow: 0 10px 20px rgba(52, 152, 219, 0.1);
        }
        
        .feature-section {
            background: linear-gradient(135deg, #ffffff 0%, #f8f9fa 100%);
            border: 2px solid #e9ecef;
            border-radius: 20px;
            padding: 30px;
            margin: 25px 0;
            box-shadow: 0 15px 30px rgba(0, 0, 0, 0.08);
        }
        
        .result-display {
            background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
            border: 3px solid #dee2e6;
            border-radius: 15px;
            padding: 25px;
            margin: 20px 0;
            max-height: 700px;
            overflow-y: auto;
            box-shadow: inset 0 5px 10px rgba(0, 0, 0, 0.05);
        }
        
        .tab-content {
            padding: 30px;
            background: rgba(255, 255, 255, 0.8);
            border-radius: 15px;
            margin: 15px 0;
        }
        
        .professional-button {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%) !important;
            border: none !important;
            border-radius: 12px !important;
            padding: 15px 25px !important;
            font-weight: bold !important;
            color: white !important;
            box-shadow: 0 8px 16px rgba(102, 126, 234, 0.3) !important;
            transition: all 0.3s ease !important;
        }
        
        .professional-button:hover {
            transform: translateY(-2px) !important;
            box-shadow: 0 12px 20px rgba(102, 126, 234, 0.4) !important;
        }
        """
        
        with gr.Blocks(
            theme=gr.themes.Soft(
                primary_hue="blue",
                secondary_hue="gray",
                neutral_hue="slate"
            ),
            title="Professional Vision Analysis Suite",
            head="<meta name='viewport' content='width=device-width, initial-scale=1'>",
            css=professional_css
        ) as interface:
            
            # Professional Header
            gr.HTML(f"""
            <div class="main-header">
                <h1 style="font-size: 2.5em; margin-bottom: 10px;">ðŸ”¬ Professional Vision Analysis Suite</h1>
                <p style="font-size: 1.3em; margin: 15px 0; opacity: 0.95;">Enterprise-Grade Multi-Model AI Platform</p>
                <p style="font-size: 1.1em; opacity: 0.9;">Gemma 3n-E4B-it â€¢ YOLTv4 â€¢ YOLOv8x</p>
                <p style="font-size: 0.95em; opacity: 0.85;">RTX A4000 Optimized â€¢ Professional Results â€¢ Research Ready</p>
            </div>
            """)
            
            # Model Selection Section
            gr.HTML('<div class="model-selector-card">')
            with gr.Row():
                with gr.Column(scale=2):
                    gr.Markdown("## ðŸŽ¯ AI Model Selection")
                    
                    model_choice = gr.Dropdown(
                        choices=self.analyzer.get_available_models(),
                        label="Select Professional AI Model",
                        info="Choose the optimal model for your vision analysis requirements",
                        scale=2,
                        container=True
                    )
                    
                with gr.Column(scale=1):
                    load_model_btn = gr.Button(
                        "ðŸš€ Load Selected Model",
                        variant="primary",
                        size="lg",
                        elem_classes=["professional-button"]
                    )
            
            model_status = gr.HTML('<div class="status-display">Select an AI model to begin professional analysis</div>')
            gr.HTML('</div>')
            
            # Professional Model Information Cards
            with gr.Row():
                with gr.Column(scale=1):
                    gr.HTML("""
                    <div class="model-info-card">
                        <h3>ðŸ¤– Gemma 3n-E4B-it</h3>
                        <p><strong>Vision-Language Excellence</strong></p>
                        <ul style="text-align: left;">
                            <li>Advanced multimodal understanding</li>
                            <li>4B effective parameters (8B total)</li>
                            <li>32K context window</li>
                            <li>Professional-grade descriptions</li>
                            <li>140+ language support</li>
                            <li>Research and analysis applications</li>
                        </ul>
                    </div>
                    """)
                
                with gr.Column(scale=1):
                    gr.HTML("""
                    <div class="model-info-card">
                        <h3>ðŸŽ¯ YOLTv4</h3>
                        <p><strong>State-of-the-Art Object Detection</strong></p>
                        <ul style="text-align: left;">
                            <li>Latest YOLT architecture</li>
                            <li>Enhanced accuracy and performance</li>
                            <li>80 COCO object classes</li>
                            <li>Professional-grade annotations</li>
                            <li>Optimized detection algorithms</li>
                            <li>Production-ready results</li>
                        </ul>
                    </div>
                    """)
                
                with gr.Column(scale=1):
                    gr.HTML("""
                    <div class="model-info-card">
                        <h3>âš¡ YOLOv8x</h3>
                        <p><strong>Proven Object Detection</strong></p>
                        <ul style="text-align: left;">
                            <li>Industry-standard performance</li>
                            <li>Robust and reliable detection</li>
                            <li>80 COCO object classes</li>
                            <li>Optimized for accuracy</li>
                            <li>High-throughput processing</li>
                            <li>Enterprise deployment ready</li>
                        </ul>
                    </div>
                    """)
            
            # Professional Description
            gr.Markdown(self.config.INTERFACE_DESCRIPTION)
            
            # Gemma 3n Interface
            with gr.Group(visible=False) as gemma_interface:
                gr.HTML('<div class="feature-section">')
                gr.Markdown("# ðŸ¤– Gemma 3n-E4B-it: Professional Vision-Language Analysis")
                
                with gr.Tabs():
                    with gr.Tab("ðŸ–¼ï¸ Single Image Analysis", elem_classes=["tab-content"]):
                        with gr.Row():
                            with gr.Column(scale=1):
                                gr.Markdown("### ðŸ“¤ Image Upload & Configuration")
                                gemma_single_image = gr.Image(
                                    type="pil",
                                    label="Upload Image for Professional Analysis",
                                    height=450,
                                    sources=["upload", "clipboard"]
                                )
                                
                                gemma_prompt = gr.Textbox(
                                    label="Custom Analysis Prompt (Optional)",
                                    placeholder="e.g., 'Provide technical analysis of composition and lighting', 'Analyze for research documentation'",
                                    lines=4,
                                    info="Leave empty for comprehensive default analysis"
                                )
                                
                                gemma_single_btn = gr.Button(
                                    "ðŸ¤– Analyze with Gemma 3n",
                                    variant="primary",
                                    size="lg",
                                    elem_classes=["professional-button"]
                                )
                            
                            with gr.Column(scale=1):
                                gr.Markdown("### ðŸ“‹ Professional Analysis Report")
                                gemma_single_result = gr.Markdown(
                                    "Upload an image to receive comprehensive AI-generated analysis with technical details",
                                    elem_classes=["result-display"]
                                )
                    
                    with gr.Tab("ðŸ“ Batch Image Analysis", elem_classes=["tab-content"]):
                        with gr.Row():
                            with gr.Column(scale=1):
                                gr.Markdown("### âš™ï¸ Batch Processing Configuration")
                                gemma_directory = gr.Textbox(
                                    label="ðŸ“ Source Directory Path",
                                    placeholder="C:\\Users\\YourName\\Pictures\\Dataset",
                                    info="Full path to directory containing images for batch analysis"
                                )
                                
                                gemma_query = gr.Textbox(
                                    label="ðŸ” Content Search Query",
                                    placeholder="e.g., 'research subjects', 'outdoor environments', 'technical equipment'",
                                    info="Natural language description of content to identify",
                                    lines=3
                                )
                                
                                gemma_batch_btn = gr.Button(
                                    "ðŸ” Process Batch with Gemma 3n",
                                    variant="primary",
                                    size="lg",
                                    elem_classes=["professional-button"]
                                )
                            
                            with gr.Column(scale=2):
                                gr.Markdown("### ðŸ“Š Batch Analysis Results")
                                gemma_batch_images = gr.Image(
                                    label="Filtered Results Grid",
                                    height=450
                                )
                                gemma_batch_text = gr.Markdown(
                                    "Configure batch settings to analyze and filter image collections",
                                    elem_classes=["result-display"]
                                )
                gr.HTML('</div>')
            
            # YOLTv4 Interface (Updated from YOLO12x)
            with gr.Group(visible=False) as yoltv4_interface:
                gr.HTML('<div class="feature-section">')
                gr.Markdown("# ðŸŽ¯ YOLTv4: State-of-the-Art Object Detection")
                
                with gr.Tabs():
                    with gr.Tab("ðŸ–¼ï¸ Single Image Detection", elem_classes=["tab-content"]):
                        with gr.Row():
                            with gr.Column(scale=1):
                                gr.Markdown("### ðŸ“¤ Image Upload for Detection")
                                yoltv4_single_image = gr.Image(
                                    type="pil",
                                    label="Upload Image for YOLTv4 Object Detection",
                                    height=450,
                                    sources=["upload", "clipboard"]
                                )
                                
                                yoltv4_single_btn = gr.Button(
                                    "ðŸŽ¯ Detect Objects with YOLTv4",
                                    variant="primary",
                                    size="lg",
                                    elem_classes=["professional-button"]
                                )
                                
                                gr.Markdown("""
                                ### ðŸ”¬ YOLTv4 Technical Specifications:
                                - **Architecture:** Latest YOLT design
                                - **Object Classes:** 80 COCO categories
                                - **Precision:** State-of-the-art accuracy
                                - **Annotations:** Professional bounding boxes
                                - **Confidence:** Reliability scoring
                                - **Output:** Publication-ready visualizations
                                """)
                            
                            with gr.Column(scale=1):
                                gr.Markdown("### ðŸ–¼ï¸ Detection Results")
                                yoltv4_single_result_image = gr.Image(
                                    label="Annotated Image with Object Detection",
                                    height=450
                                )
                                
                                gr.Markdown("### ðŸ“Š Detection Analysis Report")
                                yoltv4_single_result_text = gr.Markdown(
                                    "Upload an image to see professional object detection with bounding boxes and confidence scores",
                                    elem_classes=["result-display"]
                                )
                    
                    with gr.Tab("ðŸ“ Batch Object Detection", elem_classes=["tab-content"]):
                        with gr.Row():
                            with gr.Column(scale=1):
                                gr.Markdown("### âš™ï¸ Batch Detection Configuration")
                                yoltv4_directory = gr.Textbox(
                                    label="ðŸ“ Source Directory Path", 
                                    placeholder="C:\\Users\\YourName\\Pictures\\Dataset",
                                    info="Directory containing images for batch object detection"
                                )
                                
                                yoltv4_batch_btn = gr.Button(
                                    "ðŸŽ¯ Process All Images with YOLTv4",
                                    variant="primary",
                                    size="lg",
                                    elem_classes=["professional-button"]
                                )
                                
                                gr.Markdown("""
                                ### ðŸ“Š Batch Processing Features:
                                - **Enhanced Detection:** Latest YOLT algorithms
                                - **Statistical Analysis:** Comprehensive reports
                                - **Visual Summaries:** Professional grids
                                - **Performance Metrics:** Processing statistics
                                - **Export Ready:** Documentation-quality results
                                - **Scalable:** Optimized for large datasets
                                """)
                            
                            with gr.Column(scale=2):
                                gr.Markdown("### ðŸ“Š Batch Detection Results")
                                yoltv4_batch_images = gr.Image(
                                    label="All Images with Object Detection",
                                    height=450
                                )
                                yoltv4_batch_text = gr.Markdown(
                                    "Specify directory path to perform batch object detection across all images",
                                    elem_classes=["result-display"]
                                )
                gr.HTML('</div>')
            
            # YOLOv8x Interface
            with gr.Group(visible=False) as yolov8x_interface:
                gr.HTML('<div class="feature-section">')
                gr.Markdown("# âš¡ YOLOv8x: High-Performance Object Detection")
                
                with gr.Tabs():
                    with gr.Tab("ðŸ–¼ï¸ Single Image Detection", elem_classes=["tab-content"]):
                        with gr.Row():
                            with gr.Column(scale=1):
                                gr.Markdown("### ðŸ“¤ Image Upload for Detection")
                                yolov8x_single_image = gr.Image(
                                    type="pil",
                                    label="Upload Image for YOLOv8x Object Detection",
                                    height=450,
                                    sources=["upload", "clipboard"]
                                )
                                
                                yolov8x_single_btn = gr.Button(
                                    "âš¡ Detect Objects with YOLOv8x",
                                    variant="primary",
                                    size="lg",
                                    elem_classes=["professional-button"]
                                )
                                
                                gr.Markdown("""
                                ### âš¡ YOLOv8x Technical Specifications:
                                - **Architecture:** Proven CNN-based design
                                - **Object Classes:** 80 COCO categories
                                - **Performance:** Industry-standard accuracy
                                - **Reliability:** Robust object detection
                                - **Speed:** Optimized inference
                                - **Applications:** Enterprise deployment ready
                                """)
                            
                            with gr.Column(scale=1):
                                gr.Markdown("### ðŸ–¼ï¸ Detection Results")
                                yolov8x_single_result_image = gr.Image(
                                    label="Annotated Image with Object Detection",
                                    height=450
                                )
                                
                                gr.Markdown("### ðŸ“Š Detection Analysis Report")
                                yolov8x_single_result_text = gr.Markdown(
                                    "Upload an image to see reliable object detection with professional annotations",
                                    elem_classes=["result-display"]
                                )
                    
                    with gr.Tab("ðŸ“ Batch Object Detection", elem_classes=["tab-content"]):
                        with gr.Row():
                            with gr.Column(scale=1):
                                gr.Markdown("### âš™ï¸ Batch Detection Configuration")
                                yolov8x_directory = gr.Textbox(
                                    label="ðŸ“ Source Directory Path", 
                                    placeholder="C:\\Users\\YourName\\Pictures\\Dataset",
                                    info="Directory containing images for batch object detection"
                                )
                                
                                yolov8x_batch_btn = gr.Button(
                                    "âš¡ Process All Images with YOLOv8x",
                                    variant="primary",
                                    size="lg",
                                    elem_classes=["professional-button"]
                                )
                                
                                gr.Markdown("""
                                ### ðŸ“Š Batch Processing Features:
                                - **High Throughput:** Fast batch processing
                                - **Consistent Results:** Reliable detection
                                - **Comprehensive Reports:** Detailed statistics
                                - **Visual Documentation:** Professional output
                                - **Enterprise Scale:** Production deployment
                                - **Quality Assurance:** Proven performance
                                """)
                            
                            with gr.Column(scale=2):
                                gr.Markdown("### ðŸ“Š Batch Detection Results")
                                yolov8x_batch_images = gr.Image(
                                    label="All Images with Object Detection",
                                    height=450
                                )
                                yolov8x_batch_text = gr.Markdown(
                                    "Specify directory path to perform batch object detection across all images",
                                    elem_classes=["result-display"]
                                )
                gr.HTML('</div>')
            
            # Event Handlers
            load_model_btn.click(
                fn=self.load_model_interface,
                inputs=[model_choice],
                outputs=[model_status, gemma_interface, yoltv4_interface, yolov8x_interface]  # Updated: yoltv4_interface
            )
            
            # Gemma 3n event handlers
            gemma_single_btn.click(
                fn=self.process_single_gemma,
                inputs=[gemma_single_image, gemma_prompt],
                outputs=[gemma_single_result]
            )
            
            gemma_batch_btn.click(
                fn=self.process_batch_gemma,
                inputs=[gemma_directory, gemma_query],
                outputs=[gemma_batch_images, gemma_batch_text]
            )
            
            # YOLTv4 event handlers (Updated from YOLO12x)
            yoltv4_single_btn.click(
                fn=lambda img: self.process_single_yolo(img, "YOLTv4"),
                inputs=[yoltv4_single_image],
                outputs=[yoltv4_single_result_image, yoltv4_single_result_text]
            )
            
            yoltv4_batch_btn.click(
                fn=lambda dir: self.process_batch_yolo(dir, "YOLTv4"),
                inputs=[yoltv4_directory],
                outputs=[yoltv4_batch_images, yoltv4_batch_text]
            )
            
            # YOLOv8x event handlers
            yolov8x_single_btn.click(
                fn=lambda img: self.process_single_yolo(img, "YOLOv8x"),
                inputs=[yolov8x_single_image],
                outputs=[yolov8x_single_result_image, yolov8x_single_result_text]
            )
            
            yolov8x_batch_btn.click(
                fn=lambda dir: self.process_batch_yolo(dir, "YOLOv8x"),
                inputs=[yolov8x_directory],
                outputs=[yolov8x_batch_images, yolov8x_batch_text]
            )
            
            # Professional Footer
            gr.HTML("""
            <div style="text-align: center; margin-top: 50px; padding: 30px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 20px; color: white; box-shadow: 0 15px 30px rgba(0, 0, 0, 0.1);">
                <h3 style="margin-bottom: 15px;">ðŸš€ Professional Vision Analysis Suite v3.1</h3>
                <p style="font-size: 1.1em; margin: 10px 0;"><strong>Enterprise Models:</strong> Gemma 3n-E4B-it â€¢ YOLTv4 â€¢ YOLOv8x</p>
                <p style="font-size: 1em; margin: 10px 0;"><strong>Hardware:</strong> RTX A4000 Optimized â€¢ <strong>Performance:</strong> Professional Grade â€¢ <strong>Applications:</strong> Research & Production</p>
                <p style="font-size: 0.9em; opacity: 0.9; margin: 15px 0;">
                    <em>âš ï¸ Professional AI vision analysis system for research, development, and production environments. 
                    Results are AI-generated and should be validated for critical applications.</em>
                </p>
                <p style="font-size: 0.85em; opacity: 0.8;">
                    <strong>System Requirements:</strong> transformers â‰¥ 4.53.0 â€¢ ultralytics â€¢ PyTorch â‰¥ 2.0 â€¢ CUDA 11.8+ â€¢ RTX A4000
                </p>
            </div>
            """)
        
        return interface

def main():
    """Professional application entry point"""
    print("ðŸš€ Initializing Professional Vision Analysis Suite v3.1...")
    print("=" * 90)
    
    try:
        # Professional system validation
        print("ðŸ”§ Professional System Requirements Validation:")
        print(f"   Python Environment: {sys.version.split()[0]}")
        print(f"   PyTorch Framework: {torch.__version__}")
        print(f"   CUDA Acceleration: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name()
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"   GPU Hardware: {gpu_name}")
            print(f"   VRAM Capacity: {total_memory:.1f} GB")
            
            if total_memory >= 16.0:
                print("   âœ… Hardware: Professional grade - RTX A4000 class")
            elif total_memory >= 8.0:
                print("   âœ… Hardware: Suitable for professional use")
            else:
                print("   âš ï¸  Hardware: Limited VRAM - some features may be restricted")
        
        print(f"   Transformers Library: {transformers_version} {'âœ… Professional' if GEMMA_AVAILABLE else 'âš ï¸ Limited'}")
        print(f"   Ultralytics YOLO: {'âœ… Professional' if YOLO_AVAILABLE else 'âŒ Not Available'}")
        print(f"   Gradio Interface: {gradio_version} âœ…")
        
        # Model availability summary
        print("\nðŸŽ¯ Available AI Models:")
        if GEMMA_AVAILABLE:
            print("   ðŸ¤– Gemma 3n-E4B-it: Vision-language analysis ready")
        else:
            print("   âŒ Gemma 3n-E4B-it: Requires transformers >= 4.53.0")
        
        if YOLO_AVAILABLE:
            print("   ðŸŽ¯ YOLTv4: State-of-the-art object detection ready")  # Updated: YOLTv4
            print("   âš¡ YOLOv8x: High-performance object detection ready")
        else:
            print("   âŒ YOLO Models: Requires ultralytics installation")
        
        # Initialize professional analyzer
        analyzer = MultiModelAnalyzer()
        
        # Create professional interface
        interface_manager = ProfessionalGradioInterface(analyzer)
        app = interface_manager.create_interface()
        
        print("\nâœ… Professional Vision Analysis Suite Ready!")
        print("ðŸŒ Professional Interface: http://localhost:7860")
        print("ðŸ“± Auto-launch: Opening in default browser")
        print("\n" + "=" * 90)
        print("ðŸŽ¯ Professional Features Available:")
        print("  ðŸ¤– Gemma 3n-E4B-it: Advanced vision-language analysis with detailed reporting")
        print("  ðŸŽ¯ YOLTv4: State-of-the-art object detection with enhanced performance")  # Updated: YOLTv4
        print("  âš¡ YOLOv8x: Proven object detection with enterprise reliability")
        print("  ðŸ“Š Batch Processing: Professional-grade mass analysis capabilities")
        print("  ðŸŽ¨ Professional UI: Software-grade interface with advanced styling")
        print("  ðŸ“‹ Detailed Reports: Publication-ready analysis documentation")
        print("  ðŸ§  Memory Management: Automatic model offloading to prevent CUDA OOM")
        print("=" * 90)
        print("Press Ctrl+C to stop the professional suite")
        
        # Launch professional application
        app.launch(
            server_name="localhost",
            server_port=7860,
            share=False,
            inbrowser=True,
            show_error=True,
            favicon_path=None,
            ssl_verify=False,
            quiet=False
        )
        
    except KeyboardInterrupt:
        print("\n\nðŸ‘‹ Professional Vision Analysis Suite stopped by user")
    except Exception as e:
        print(f"\nâŒ Critical system error: {e}")
        print("\nðŸ”§ Professional Support:")
        print("1. Verify requirements: pip install transformers>=4.53.0 ultralytics gradio")
        print("2. Validate GPU: Ensure CUDA drivers and RTX A4000 accessibility")
        print("3. Check models: Verify model file paths and permissions")
        print("4. Review logs: Check system logs for detailed error information")

if __name__ == "__main__":
    # Professional directory initialization
    try:
        os.makedirs("models", exist_ok=True)
        os.makedirs(os.path.join("models", "gemma-3n-e4b-it"), exist_ok=True)
        print("ðŸ“ Professional directory structure initialized")
    except Exception as e:
        print(f"Warning: Directory initialization issue: {e}")
    
    # Launch professional suite
    main()
